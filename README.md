# Тестовое задание (Playwright, RabbitMQ)

Парсер видео с каналов YouTube.

Стек:
- Python 3.11.
- Управление браузером: Playwright (асинхронный api).
- Хранилище: PostgreSQL (asyncpg).
- Брокер очередей: RabbitMQ (aio-pika).
- Запуск: Docker, Docker-compose.
- asyncio, pydantic-settings, msgspec.
- Управление зависимостями: Poetry.
- Линтер, форматтер: ruff.


## Краткое описание

Архитектурно система делится на два компонента: **Crawler** и **Loader**.

**Crawler** (Сборщик) асинхронно, в разных вкладках парсит данные и записывает в Rabbit.

**Loader** (Загрузчик) читает распаршенные данные из очереди и пишет в PostgreSQL.

Чтение и запись поддерживается в любом количестве вкладок (параметр **BROWSER_MAX_WORKERS**).
Он же отвечает за максимальное количество соединений в пуле PostgreSQL.

## Системные ограничения

Playwright поддерживает работу в Docker **ТОЛЬКО** в **Headless** режиме. 
Для его включения параметр **BROWSER_HEADLESS** должен быть **True**.

## Quickstart

**Headless** (без браузерного окна):
1. `mv .env.template .env`
2. `docker compose up`
3. Долго-долго ждать. Playwright устанавливается в образ Докера около 3 минут.

**Headful** (с браузерным окном):
1. `mv .env.template .env`
2. Запустить loader командой `docker compose up loader_app`. В консоли будет сообщение 
3. Открыть **.env**. Выставить: 
   - `BROWSER_HEADLESS=False`
   - `PG_HOST=localhost`
   - `RABBIT_HOST=localhost`
4. Установить зависимости через poetry: `poetry install --no-root`
5. Установить playwright: `playwright install`
6. Запустить Crawler: `poetry run crawler/run.py`

## Изменение списка ссылок на каналы

Список ссылок можно посмотреть в [write_links.py](crawler%2Fwrite_links.py).
С причинами отсутствия гибкой конфигурации списка ссылок можно ознакомиться там же.

## Описание основных концепций

### PostgreSQL
Базовые DDL-операции по созданию структуры происходит через монтирование [init.sql](init.sql) в Docker Compose.
От себя добавил поля **created_at** (время создания записи) и **updated_at** (время обновление записи). Они проставляются автоматически.

Обновление **updated_at** происходит через триггер. Создаётся также в [init.sql](init.sql).
Запись новых данных осуществляется с помощью создания временной таблицы и её MERGE с основной.

### Подгрузка страниц
Playwright иногда присылает неполный список видео. Это не было частью задания, но я немного поработал над этим.
Дополнительный таймаут после загрузки `BROWSER_PAGE_ADDITIONAL_LOADING_TIME_MS` позволяет получить больше данных.

Кроме того, иногда происходят запросы на гугл-сервисы, из-за чего вкладка бесконечно грузится и вылетает по таймауту.
Частично решить проблему удалось с помощью их блокировки. `BROWSER_REQUEST_ABORTION_CODE` - код, имитируемый от них, можно брать из [доки](https://playwright.dev/python/docs/api/class-route#route-abort-option-error-code).

Иногда переходы по ссылке всё-таки отлетают по таймауту. 
Страница в таких случаях уже получена, поэтому просто мьючу ошибку и обрабатываю страницу.

### Асинхронность
Самой медленной частью системы является Playwright и получение данных из сети.
Для решения проблемы, я использовал официальный асинхронный модуль Playwright.

При запуске всё работает так:
- Создаётся один браузер.
- В рамках одной TaskGroup создаются воркеры-таски в количестве `BROWSER_MAX_WORKERS`
- У каждого из воркеров есть своя вкладка в браузере и доступ к: входной очереди (ссылки на каналы) и выходной очереди (списки ссылок на видео с канала).
- Воркеры работают бесконечно, вычитывая одну очередь и перекладывая в другую.

Таким образом всё выполняется в рамках одного потока и процесса. И без простоя, который был бы при полной параллельности.

### RabbitMQ
Для каждого канала **Сборщиком** в Кролика  передаётся только **список id видео** (не включая "v="), а в хедерах **username канала**, начиная с `@`.
**Загрузчик** при прочтении самостоятельно собирает ссылки на видео.

Это уменьшает объём сообщений, которые нужно сериализовать, отправить, принять, десериализовать.
